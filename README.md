# Rethinking Deep Learning Generalization: A Challenge to Zhang et al. (2017)

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

This repository contains a comprehensive experimental framework to challenge and extend the findings of Zhang et al. (2017) "Understanding Deep Learning Requires Rethinking Generalization". 

**Core Thesis**: Neural networks inherently learn smooth, generalizable functions even when trained on random labels. The apparent "memorization" is actually the network finding the smoothest function consistent with unrealizable data.

## Key Contributions

1. **Baseline Replication**: Reproduces Zhang et al.'s core findings on CIFAR-10
2. **Smoothness Analysis**: Quantifies function smoothness using multiple metrics
3. **Two-Stage Learning** (Critical): Demonstrates that networks trained on random noise learn generalizable functions that other networks can efficiently learn
4. **Frequency Analysis**: Analyzes learned functions in the frequency domain
5. **Progressive Corruption**: Studies the relationship between label corruption and generalization
6. **Random Walk Learning** (New): Tests learnability on data with drastically reduced Lipschitz continuity to validate the role of smoothness

## Repository Structure

```
rethinking-generalization-rebuttal/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ paper/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ latex/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ baseline_replication.py
â”‚   â”‚   â”œâ”€â”€ smoothness_analysis.py
â”‚   â”‚   â”œâ”€â”€ two_stage_learning.py
â”‚   â”‚   â”œâ”€â”€ two_stage_learning_toy.py
â”‚   â”‚   â”œâ”€â”€ frequency_analysis.py
â”‚   â”‚   â”œâ”€â”€ complexity_measures.py
â”‚   â”‚   â””â”€â”€ random_walk_learning.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ architectures.py
â”‚   â”‚   â””â”€â”€ training.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ visualization.py
â”‚   â”‚   â””â”€â”€ statistical_tests.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_generation.py
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ figure_generation.ipynb
â”‚   â””â”€â”€ random_walk_analysis.ipynb
â””â”€â”€ results/
    â”œâ”€â”€ raw_data/
    â”œâ”€â”€ processed/
    â””â”€â”€ figures/
```

## Installation

### Requirements

- Python 3.9+
- CUDA-capable GPU (recommended, 8GB+ VRAM)
- ~10GB disk space for models and results

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/rethinking-generalization-rebuttal.git
cd rethinking-generalization-rebuttal

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Or install in development mode
pip install -e .
```

## Quick Start

### ðŸš€ Ultra-Quick Start: Toy Version (2 minutes!)

Test the core concept with a simplified version:

```bash
# Option 1: Command line
python src/experiments/two_stage_learning_toy.py

# Option 2: Jupyter notebook (recommended)
jupyter notebook notebooks/toy_two_stage_demo.ipynb
```

**Features:**
- Small dimensions (100 instead of 3072)
- Small MLP (3 layers: 256-128-64)  
- Fast training (~2-3 minutes on CPU)
- Full visualization capabilities

See [TOY_EXPERIMENT_GUIDE.md](TOY_EXPERIMENT_GUIDE.md) for details.

### Run All Experiments

```bash
# 1. Baseline replication (Zhang et al.)
python src/experiments/baseline_replication.py

# 2. Smoothness analysis
python src/experiments/smoothness_analysis.py

# 3. Two-stage learning (KEY EXPERIMENT)
python src/experiments/two_stage_learning.py

# 4. Frequency analysis
python src/experiments/frequency_analysis.py

# 5. Progressive corruption
python src/experiments/complexity_measures.py

# 6. Random walk learning (NEW!)
python src/experiments/random_walk_learning.py
```

### Generate Figures

```bash
# Open Jupyter notebook
jupyter notebook notebooks/figure_generation.ipynb
```

### Quick Test Run

For a quick test with reduced parameters:

```python
from src.experiments.baseline_replication import run_baseline_replication

# Run with single architecture and seed
results = run_baseline_replication(
    architectures=['resnet18'],
    seeds=[42],
    epochs=50,  # Reduced for testing
    verbose=True
)
```

## Experiments

### Experiment 1: Baseline Replication

Replicates Zhang et al.'s core findings:
- Train ResNet-18, VGG-11, and MLP on CIFAR-10
- Compare training with true labels vs random labels
- Both achieve ~0 training error
- True labels: ~90% test accuracy
- Random labels: ~10% test accuracy (random chance)

**Run:**
```bash
python src/experiments/baseline_replication.py
```

### Experiment 2: Smoothness Analysis

Measures function smoothness using:
- Gradient norm: ||âˆ‡f(x)||
- Lipschitz constant estimation
- Spectral norm of weight matrices
- Path norm
- Local variation

**Key Finding**: Models trained on random labels are less smooth but still significantly smoother than truly random functions.

**Run:**
```bash
python src/experiments/smoothness_analysis.py
```

### Experiment 3: Two-Stage Learning (Critical)

**This is the key novel experiment!**

**Stage 1**: Train Network_1 on random noise images with random labels
**Stage 2**: Train Network_2 to mimic Network_1's function using NEW random images

**Key Finding**: Network_2 achieves >85% agreement with Network_1's predictions on held-out test data, demonstrating that Network_1 learned a generalizable smooth function despite being trained on "unrealizable" random labels.

**Implications**: 
- Neural networks don't just memorize; they find smooth functions
- The inductive bias toward smoothness is incredibly strong
- "Memorization" is actually structured function approximation

**Run:**
```bash
python src/experiments/two_stage_learning.py
```

### Experiment 4: Frequency Analysis

Analyzes learned functions in frequency domain:
- 2D Fourier transforms
- Directional frequency analysis
- High vs low frequency energy

**Key Finding**: Models trained on random labels have more high-frequency content but still predominantly learn low-frequency functions.

**Run:**
```bash
python src/experiments/frequency_analysis.py
```

### Experiment 5: Progressive Corruption

Studies the relationship between label corruption and generalization:
- Train with 0%, 25%, 50%, 75%, 100% label corruption
- Measure test accuracy and smoothness at each level

**Key Finding**: Strong correlation between realizability and generalization, with smooth degradation as corruption increases.

**Run:**
```bash
python src/experiments/complexity_measures.py
```

### Experiment 6: Random Walk Learning

Trains networks on data with **drastically reduced Lipschitz continuity**:
- Each image differs from the previous by exactly 1 pixel
- Labels are randomized at each step
- Creates a "random walk" through image space

**Two-Stage Variants:**
- **Variant A**: Network_2 trained on walk data labeled by Network_1
- **Variant B**: Network_2 trained on uniform random data labeled by Network_1

**Hypothesis**: Random walk data (lower Lipschitz continuity) should be significantly harder to learn/generalize compared to uniform random noise.

**Key Findings**: 
- Network can achieve high training accuracy on walk data
- Test generalization remains poor (~10% on both walk continuation and uniform random)
- Two-stage learning reveals differences in learnability between walk and uniform data
- Validates that Lipschitz continuity matters for function learnability

**Run:**
```bash
python src/experiments/random_walk_learning.py
```

**Analyze:**
```bash
jupyter notebook notebooks/random_walk_analysis.ipynb
```

## Key Results

### Main Findings

1. **Networks Fit Random Labels**: Confirmed Zhang et al.'s finding âœ“
2. **But Learn Smooth Functions**: Random-label models are still remarkably smooth
3. **Generalizable "Random" Functions**: Network_2 learns Network_1's function efficiently
4. **Sample Efficiency**: Network_2 needs 10x fewer samples than Network_1
5. **Frequency Preference**: Strong bias toward low-frequency functions

### Critical Insight

The fact that a second network can generalize to a first network's function (trained on random labels) demonstrates that:
- The first network did NOT memorize arbitrarily
- It learned a smooth, structured function
- This smoothness emerges from the architecture and optimization, not the data

This challenges the interpretation that random label fitting represents "pure memorization."

## Computational Requirements

- **GPU**: NVIDIA GPU with 8GB+ VRAM (recommended)
- **CPU**: Works but slower (~10x)
- **Time**: 
  - Single model: ~2 hours on GPU
  - Full experiments: ~100 hours on GPU
  - Quick test: ~10 minutes
- **Storage**: ~10GB for all models and results

## Citation

If you use this code, please cite:

```bibtex
@software{rethinking_generalization_2024,
  title = {Rethinking Deep Learning Generalization: A Challenge to Zhang et al.},
  author = {Your Name},
  year = {2024},
  url = {https://github.com/yourusername/rethinking-generalization-rebuttal}
}
```

Original paper being challenged:
```bibtex
@inproceedings{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={ICLR},
  year={2017}
}
```

## Project Structure Details

### `src/models/`
- `architectures.py`: ResNet-18, VGG-11, MLP implementations
- `training.py`: Training loop and utilities

### `src/experiments/`
All experimental scripts with command-line interfaces

### `src/analysis/`
- `metrics.py`: Smoothness and generalization metrics
- `visualization.py`: Publication-quality plotting
- `statistical_tests.py`: Statistical analysis tools

### `src/utils/`
- `config.py`: Centralized configuration
- `data_generation.py`: Data loading and generation

### `notebooks/`
- `exploratory_analysis.ipynb`: Interactive exploration
- `figure_generation.ipynb`: Generate all paper figures

## Configuration

Edit `src/utils/config.py` to customize:
- Training hyperparameters
- Data parameters
- Analysis parameters
- Paths

## Troubleshooting

### CUDA Out of Memory
Reduce batch size in `src/utils/config.py`:
```python
BATCH_SIZE = 64  # Default: 128
```

### Slow Training
- Ensure CUDA is available: `torch.cuda.is_available()`
- Reduce number of epochs for testing
- Use fewer seeds

### Missing Results
Ensure you've run the prerequisite experiments in order:
1. Baseline replication (required for smoothness and frequency analysis)
2. Smoothness analysis
3. Two-stage learning (independent)

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## License

MIT License - see LICENSE file for details

## Acknowledgments

- Original Zhang et al. (2017) paper for foundational insights
- PyTorch team for the framework
- CIFAR-10 dataset creators

## Contact

For questions or collaborations:
- Email: your.email@example.com
- Issues: GitHub Issues page

---

**Note**: This is a research project. Results may vary with different random seeds, architectures, and hyperparameters. For reproducibility, use the provided random seeds.

